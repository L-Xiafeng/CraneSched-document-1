{"./":{"url":"./","title":"1 CraneSched简介","keywords":"","body":"CraneSched 简介 鹤思(CraneSched)是北京大学计算中心和北京大学长沙计算与数字经济研究院联合研发的算力中心智能调度系统。通过对高性能计算和智能计算两类作业场景进行细化建模与针对性优化，实现高性能计算与智能计算场景下的资源管理、作业管理、资源隔离、通讯加密等功能。 CraneSched分为前端和后端，前端主要是用Go开发，后端主要是用C++开发，欢迎开发者一起加入。 CraneSched 在线试用地址 ：CraneSched -Demo CraneSched前端开源地址：CraneSched -FrontEnd CraneSched 后端开源地址：CraneSched CraneSched 文档地址：CraneSched-Document CraneSched 设计理念 完全开源：源代码开放可得； 功能强大：支持高性能计算与智能计算两种作业模式； 调度高效：支持每秒调度10000个作业以上，支持同时运行300万个作业以上； 高度可伸缩：支持100000个节点以上的算力中心规模； 安全可靠：RBAC权限控制和通信加密； 代码精炼：代码精炼高效，可读性高； 简洁易用：用户使用指令和系统管理指令简洁易用； 高度可容错：作业故障自动恢复、无单点故障、系统状态快速故障恢复。 CraneSched 架构 针对算力中心的调度场景，CraneSched采用中心化管理模式，Cranectld是部署在主控节点的守护进程，也是调度系统的“大脑”。Craned是部署在计算节点上的守护进程，也是调度系统下达给计算节点的指令的执行者。 Cranectld负责算力中心节点生命周期的管理、作业队列的调度及管理、节点资源调度及管理，处理来自用户指令的作业提交、修改、查询等请求。Craned主要用来监控节点资源及作业状态，接受用户的各种指令，并将其发送给Cranectld，向用户传送Cranectld对指令处理的返回结果。 面向算力网络的国家战略，鹤思智能调度系统计划采用两级调度架构，上层调度为XCraneSched，下层调度为CraneSched，两者结合至上而下解决算力网络中算力资源调度问题。CraneSched针对单个算力中心资源调度，主要运行高性能计算和智能计算作业，通过适配器与XCraneSched连接，并承接来自XCraneSched分派的作业。XCraneSched通过各种适配器去连接超算、智算、云计算等各类算力中心，将其汇聚成一张算力网络，接受用户提交的作业，并将作业分发到最“合适”的算力中心。 "},"features.html":{"url":"features.html","title":"2 CraneSched功能和特点","keywords":"","body":"CraneSched功能和特点 资源管理功能 资源查看 资源调度 资源管理（增加/减少/节点状态变化等） 资源审计（节点状态变化事件记录） 作业管理功能 作业提交 作业取消 作业修改，时间延长等 作业状态查询 作业审计 特点 完全开源 支持高性能计算与智能计算两种作业模式 高可靠故障恢复，容错性高 资源和节点状态可持续化，宕机之后重启自动恢复 高度可扩展性，可以同时支持提交/取消/管理多个独立作业 高性能，每秒可处理多个作业，为作业分配资源并运行 常用命令 cinfo： 查看节点与分区状态 cbatch： 提交批量处理作业 crun： 提交交互式任务 calloc： 提交交互式任务 cqueue： 查看作业队列 ccancel： 取消运行或提交的作业 ccontrol： 查看/修改分区和节点状态 cacctmgr： 查看和调整用户/账号信息 cacct： 查看作业信息 常用术语 job： 作业 node： 计算节点 core： CPU核 tasks： 任务数，一般一个任务使用一个CPU核，可理解为作业所需的CPU核数 partition： 分区 user： 用户名 account： 账户 stdout： 标准输出文件，程序运行正常时输出信息的文件，一般指输出到屏幕的信息 stderr： 标准错误文件，程序运行错误时输出信息的文件，一般指输出到屏幕的信息 "},"config/":{"url":"config/","title":"3 CraneSched安装配置","keywords":"","body":"CraneSched安装教程 CraneSched分为前端和后端，前端主要是用Go语言开发，后端主要用C++语言开发，部署CraneSched项目需要分为前端和后端两部分分别部署构建。 Crane-CentOS7 Crane-Rocky_Linux9 Crane-ubuntu Crane-OpenEuler22 CraneSched配置文件 "},"config/Crane-CentOS7.html":{"url":"config/Crane-CentOS7.html","title":"3.1 Crane-CentOS 7 环境配置","keywords":"","body":"Crane后端环境配置-CentOS 7 以下内容为配置代码编译环境，在编译项目的节点执行，假设用户为root 1.换源（仅针对北大校内机器） # 清除所有源_c rm -rf /etc/yum.repos.d/* cat > /etc/yum.repos.d/CentOS-Base.repo 重建缓存 yum makecache 安装其他扩展仓库，如果已经装了这两个要先卸载！！ yum install -y epel-release centos-release-scl-rh 更换epel仓库为北大镜像源，修改repo文件/etc/yum.repos.d/epel.repo cat > /etc/yum.repos.d/epel.repo 更改scl仓库为北大镜像源，修改repo文件 cat > /etc/yum.repos.d/CentOS-SCLo-scl-rh.repo 重建缓存 yum makecache 2.环境准备 关闭SeLinux #重启后失效 setenforce 0 #重启后生效 sed -i s#SELINUX=enforcing#SELINUX=disabled# /etc/selinux/config 2.1 安装ca-certificates，确保系统能够安全地与外部服务器进行通信 yum -y install ca-certificates 2.2 同步时钟 yum -y install ntp ntpdate systemctl start ntpd systemctl enable ntpd timedatectl set-timezone Asia/Shanghai 2.3 关闭防火墙，不允许关闭防火墙则考虑开放10011、10010、873端口 systemctl stop firewalld systemctl disable firewalld # 上述两条命令不成功需要执行下面命令 #或者开放端口 firewall-cmd --add-port=10011/tcp --permanent --zone=public firewall-cmd --add-port=10010/tcp --permanent --zone=public firewall-cmd --add-port=873/tcp --permanent --zone=public # 重启防火墙(修改配置后要重启防火墙) firewall-cmd --reload 3. 安装运维工具链 yum install -y tig tmux fish pdsh htop 4.安装依赖包 yum install -y openssl-devel libcgroup-devel curl-devel pam-devel zlib-devel zlib-static libaio-devel # 补充：libcgroup-devel的安装需要重新编译一下libcgroup文件 wet https://github.com/libcgroup/libcgroup/releases/download/v3.1.0/libcgroup-3.1.0.tar.gz tar -zxvf libcgroup-3.1.0.tar.gz cd libcgroup-3.1.0 dnf install tar bison flex systemd-devel -y sudo ./configure make -j 12 sudo make install 5.安装工具链 工具链版本 cmake版本>=3.24 libstdc++版本>= 11 如果安装clang，版本>= 19 如果安装g++,版本>= 135.1 所有系统适用工具链 安装wget，tar sudo dnf install -y wget sudo dnf install -y tar 安装cmake，选择一个合适的源码存放位置，从github下载源码 wget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4-linux-x86_64.sh 执行cmake安装脚本 bash cmake-3.26.4-linux-x86_64.sh --prefix=/usr/local --skip-license 检查cmake安装是否成功 cmake --version #cmake version 3.26.4 5.2 CentOS7 其他工具链包 5.2.1 命令行安装 gcc 13 wget https://ftp.gnu.org/gnu/gcc/gcc-13.2.0/gcc-13.2.0.tar.gz tar -zxvf gcc-13.2.0.tar.gz yum install -y bzip2 cd gcc-13.2.0 ./contrib/download_prerequisites mkdir build cd build/ ../configure -enable-checking=release -enable-languages=c,c++ -disable-multilib make -j 12 sudo make install 5.2.2 yum安装 安装其他工具链包 yum install -y ninja-build patch devtoolset-11 rh-git218 yum install -y devtoolset-11-libasan-devel devtoolset-11-libtsan-devel 为了避免每次手动生效，可以在~/.bash_profile中设置 vim ~/.bash_profile source scl_source enable devtoolset-11 source scl_source enable rh-git218 然后重启终端，这时用gcc --version命令查询，可以看到版本已经是11.2系列了 gcc (GCC) 11.2.1 20220127 (Red Hat 11.2.1-9) Copyright (C) 2021 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. 6. 编译Crane程序 #不必要配置代理 git config --global http.proxy http://crane:hf2lH9UUC3E0@192.168.1.1:7890 git config --global https.proxy http://crane:hf2lH9UUC3E0@192.168.1.1:7890 # 选择一个合适的位置克隆项目 git clone https://github.com/PKUHPC/Crane.git cd Crane mkdir build cd build # 首次编译需要下载第三方库，耗时较长 cmake -G Ninja -DCMAKE_C_COMPILER=/opt/rh/devtoolset-11/root/usr/bin/gcc -DCMAKE_CXX_COMPILER=/opt/rh/devtoolset-11/root/usr/bin/g++ .. cmake --build . --target cranectld craned pam_crane # 仅安装到本机，craned节点需手动scp ninja install 7. Pam模块（最后节点部署成功再操作，否则会出现ssh不上远程主机的情况） 首次编译完成后需要将pam模块动态链接库放入系统指定位置 cp Crane/build/src/Misc/Pam/pam_Crane.so /usr/lib64/security/ #如果不行就在Pam目录下查找有可能生成so名字大小写不一致 cp Crane/build/src/Misc/Pam/pam_crane.so /usr/lib64/security/ 在 /etc/pam.d/sshd 中添加红色行： #%PAM-1.0 auth required pam_sepermit.so auth substack password-auth auth include postlogin # Used with polkit to reauthorize users in remote sessions -auth optional pam_reauthorize.so prepare account required pam_crane.so account required pam_nologin.so account include password-auth password include password-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open env_params session required pam_namespace.so session optional pam_keyinit.so force revoke session include password-auth session required pam_crane.so session include postlogin # Used with polkit to reauthorize users in remote sessions -session optional pam_reauthorize.so prepare 注意：session optional pam_crane.so必须位于 session include password-auth之后！因为password-auth中有pam_systemd.so这个模块，会导致sshd session被移入systemd:/user.slice这个cgroups中！ 8. 安装mongodb 安装数据库仅在需要存储数据的节点安装 修改mongodb的yum源： cat > /etc/yum.repos.d/mongodb-6.0.2.repo 补充：若爆出-bash: /etc/yum.repos.d/mongodb-6.0.2.repo: Permission denied可使用下方命令： sudo tee /etc/yum.repos.d/mongodb-6.0.2.repo 安装并添加mongodb开机启动 yum install mongodb-org -y # 添加开机启动 systemctl enable mongod systemctl start mongod 安装完mongod用户的home目录为/var/lib/mongo 利用openssl在/var/lib/mongo生成密钥文件 openssl rand -base64 756 | sudo -u mongod tee /var/lib/mongo/mongo.key sudo -u mongod chmod 400 /var/lib/mongo/mongo.key 创建用户 # 从mongodb6.0开始，mongo mongosh # 进入mongodb之后进行下列操作 use admin # user: 用户名 pwd：密码 roles：root 代表超級管理员权限 admin代表给admin数据库加的超级管理员 db.createUser({ user:'admin', pwd:'123456', roles:[{ role:'root',db:'admin'}] }) # 重启前先关闭服务器 db.shutdownServer() quit 修改/etc/mongod.conf配置文件，开启权限验证，并配置副本集配置 vim /etc/mongod.conf ...... #开启权限验证 security: authorization: enabled keyFile: /var/lib/mongo/mongo.key replication: #副本集名称,crane的配置文件要与此一致 replSetName: crane_rs 重新启动mongodb数据库 systemctl restart mongod 进入mongosh，初始化副本集 mongosh use admin db.auth(\"admin\",\"123456\") // 如果不需要配置外部连接，并且副本集只有该主机一个节点，config可不配置 config = { \"_id\": \"crane_rs\", // 注意名称一致 \"members\": [ { \"_id\": 0, \"host\": \":27017\" // 建议这里填写部署数据集的节点主机名，默认为127.0.0.1 } // ... 其他节点（如果有的话） ] } rs.initiate() 9. 运行项目 首先根据自身的集群情况在配置文件当中进行相应配置，配置文件样例保存在etc/crane/config.yaml.example mkdir -p /etc/crane cp etc/config.yaml.example /etc/crane/config.yaml #上述命令报错就按照如下输入 cp etc/config.yaml /etc/crane/ vim /etc/crane/config.yaml 直接执行可执行文件启动 此时目录应该在项目根目录 cd build/src # Cranectld启动命令 CraneCtld/cranectld # Craned启动命令 Craned/craned Systemctl 启动服务 (现在这个用不了！) systemctl start cranectld # 控制节点守护程序服务 systemctl start craned # 计算节点守护程序服务 10. 其他节点环境部署 10.1 SCP命令版本 计算节点部署项目，无需编译项目，仅需复制相应的执行文件和配置文件即可 # 比如配置计算节点crane02 ssh crane02 \"mkdir -p /etc/crane\" scp /usr/local/bin/craned crane02:/usr/local/bin/ scp /etc/systemd/system/craned.service crane02:/etc/systemd/system/ scp /etc/crane/config.yaml crane02:/etc/crane/ 10.2 PDSH版本 如果没安装pdsh，安装pdsh yum install -y pdsh 更新cranectld # 注意需位于编译路径下执行 pdsh -w cranectl systemctl stop cranectld pdcp -w cranectl src/CraneCtld/cranectld /usr/local/bin pdsh -w cranectl systemctl start cranectld 更新craned pdsh -w crane0[1-4] systemctl stop craned pdcp -w crane0[1-4] Craned/craned /usr/local/bin pdsh -w crane0[1-4] systemctl start craned 11. 其他配置 11.1 Fish shell cd /etc/yum.repos.d/ wget https://download.opensuse.org/repositories/shells:/fish:/release:/3/CentOS_7/shells:fish:release:3.repo yum makecache yum install -y fish 11.2 Nix 见 https://mirrors.tuna.tsinghua.edu.cn/help/nix/ 附：多节点环境部署说明 scp 在计算节点部署 Craned 时无需完整编译项目，仅需复制相应的可执行文件和配置文件： # 例如配置计算节点 crane02 ssh crane02 \"mkdir -p /etc/crane\" scp /usr/local/bin/craned crane02:/usr/local/bin/ scp /etc/systemd/system/craned.service crane02:/etc/systemd/system/ scp /etc/crane/config.yaml crane02:/etc/crane/ 注意：计算节点仍需完成“安装项目依赖”中的 libcgroup 安装部分。 Crane 前端环境配置-CentOS 7 理论上在任何使用 systemd 的系统上都能使用（例如 Debian/Ubuntu/AlmaLinux/Fedora 等）。该教程涉及的软件基于 ARM64。如果使用 x86-64 等架构，请调整软件下载链接。请全程以 root 用户执行命令。建议先完成后端环境的安装。 1. 安装 Golang GOLANG_TARBALL=go1.22.0.linux-amd64.tar.gz curl -L https://go.dev/dl/${GOLANG_TARBALL} -o /tmp/go.tar.gz # 移除旧版本的 Golang 环境 rm -rf /usr/local/go tar -C /usr/local -xzf /tmp/go.tar.gz && rm /tmp/go.tar.gz echo 'export GOPATH=/root/go' >> /etc/profile.d/go.sh echo 'export PATH=$GOPATH/bin:/usr/local/go/bin:$PATH' >> /etc/profile.d/go.sh echo 'go env -w GO111MODULE=on' >> /etc/profile.d/go.sh echo 'go env -w GOPROXY=https://goproxy.cn,direct' >> /etc/profile.d/go.sh source /etc/profile.d/go.sh go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 2. 安装 Protoc PROTOC_ZIP=protoc-23.2-linux-x86_64.zip \\ curl -L https://github.com/protocolbuffers/protobuf/releases/download/v23.2/${PROTOC_ZIP} -o /tmp/protoc.zip unzip /tmp/protoc.zip -d /usr/local rm /tmp/protoc.zip /usr/local/readme.txt 3. 拉取项目 git clone https://github.com/PKUHPC/CraneSched-FrontEnd.git 4. 编译项目并部署前端 工作目录为CraneSched-FrontEnd，在该目录下编译所有 Golang 组件并安装。 cd CraneSched-FrontEnd make make install 5. 启动 Cfored 和 Cplugind（可选） 如果需要提交交互式任务（crun, calloc），则需要启用 Cfored： # 设置开机启动 systemctl enable cfored systemctl start cfored 如果配置文件中启用了插件系统，则需要启用 Cplugind： # 设置开机启动 systemctl enable cplugind systemctl start cplugind 6. 安装 Cwrapper 别名（可选） 可以通过下列命令安装 Crane 的 Slurm 别名，从而支持使用 Slurm 的命令形式使用 Crane： cat > /etc/profile.d/cwrapper.sh "},"config/Crane-Rocky_Linux9.html":{"url":"config/Crane-Rocky_Linux9.html","title":"3.2 Crane-Rocky_Linux 9 环境配置","keywords":"","body":"Crane 后端环境配置 - Rocky Linux 9 该教程基于 Rocky Linux 9，但也基本适用于 Rocky Linux 8 / Fedora。该教程涉及的软件基于 x86-64。如果使用 ARM64 等架构，请调整软件下载链接。请全程以 root 用户执行命令。 1. 环境准备 1.1 添加 EPEL 软件源 dnf install -y yum-utils epel-release dnf config-manager --set-enabled crb dnf update -y # 建议重启以应用更新后的内核 reboot 1.2 启用时间同步 dnf install -y chrony systemctl restart systemd-timedated timedatectl set-timezone Asia/Shanghai timedatectl set-ntp true 1.3 关闭防火墙 systemctl stop firewalld systemctl disable firewalld 若集群不允许关闭防火墙，则考虑开放 10010、10011、10012、873 端口。 firewall-cmd --add-port=10012/tcp --permanent --zone=public firewall-cmd --add-port=10011/tcp --permanent --zone=public firewall-cmd --add-port=10010/tcp --permanent --zone=public firewall-cmd --add-port=873/tcp --permanent --zone=public firewall-cmd --reload 注意：若有多个节点，需在每个节点上执行此步骤，否则无法进行节点间的通信。 1.4 关闭SELinux #重启后失效 setenforce 0 #重启后生效 sed -i s#SELINUX=enforcing#SELINUX=disabled# /etc/selinux/config 1.5 切换CGroup 版本 Rocky 9 默认使用 CGroup v2。CraneSched CGroup v2支持需要额外配置 1. CgroupV1切换和配置使用CgroupV1需按照以下步骤切换版本： # 设置内核启动参数，更改 CGroup 版本。 grubby --update-kernel=/boot/vmlinuz-$(uname -r) --args=\"systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller\" # 重启系统生效 reboot # 验证版本 mount | grep cgroup 2. CGroup V2配置 需要注意 root cgroup 下的子 cgroup 有无对相关资源的操作权限 # 检查子cgroup是否有相关资源的权限，如输出 cpu io memory 等 cat /sys/fs/cgroup/cgroup.subtree_control # 设置子组的权限 echo '+cpuset +cpu +io +memory +pids' > /sys/fs/cgroup/cgroup.subtree_control 注意：CGroup V2 对 device 的控制与 V1 使用的机制不同，V2 使用 eBPF 控制设备访问，在 CGroup V2 eBPF对 bpf 程序进行配置 CGroup V2 eBPF eBpf需要使用clang编译，请确保系统中有clang17及以上 1. clang19安装教程： dnf install \\ bpftool \\ bcc \\ bcc-tools \\ elfutils-libelf-devel \\ zlib-devel # 源码编译安装 clang19 git clone --depth=1 --branch llvmorg-19.1.0 https://github.com/llvm/llvm-project.git \\ llvm-project-19.1.0 cd llvm-project-19.1.0/ dnf install -y libedit-devel ncurses-devel libxml2-devel python3-devel swig mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX='/usr/local' \\ -DCMAKE_BUILD_TYPE='Release' -G Ninja \\ -DLLVM_ENABLE_PROJECTS='clang;clang-tools-extra;lld;lldb' -DLLVM_ENABLE_RUNTIMES=all \\ -DLLVM_TARGETS_TO_BUILD='X86;BPF' ../llvm ninja && ninja install cd ../ mkdir build-libcxx && cd build-libcxx cmake -G Ninja -DCMAKE_INSTALL_PREFIX='/usr/local' -DCMAKE_C_COMPILER=clang \\ -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_BUILD_TYPE=Release -S ../runtimes \\ -DLLVM_ENABLE_RUNTIMES=\"libcxx;libcxxabi;libunwind\" ninja cxx cxxabi unwind #ninja check-cxx check-cxxabi check-unwind ninja install-cxx install-cxxabi install-unwind # Install asan and tsan header and libs for develop build cd ../ mkdir build-compiler-rt && cd build-compiler-rt cmake ../compiler-rt -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ \\ -DCMAKE_INSTALL_PREFIX='/usr/local' -DCMAKE_BUILD_TYPE='Release' -G Ninja \\ -DLLVM_CMAKE_DIR=../cmake/modules ninja install # 下载并编译安装 libbpf wget https://github.com/libbpf/libbpf/archive/refs/tags/v1.4.6.zip unzip v1.4.6.zip cd libbpf-1.4.6/src # 编译并安装 make make install 2. ebpf 系统配置 # 在项目build目录下 cp ./src/Misc/BPF/cgroup_dev_bpf.o /etc/crane/cgroup_dev_bpf.o # 检查子cgroup是否有相关资源的权限，如输出 cpu io memory 等 cat /sys/fs/cgroup/cgroup.subtree_control # 设置子组的权限 echo '+cpuset +cpu +io +memory +pids' > /sys/fs/cgroup/cgroup.subtree_control # 出现 bpf load失败解决方式： # 挂载 bpf 文件系统 mount -t bpf bpf /sys/fs/bpf #挂载 bpf 调试文件 mount -t debugfs none /sys/kernel/debug # 使用'cat /sys/kernel/debug/tracing/trace_pipe'查看设备访问日志 3. BPF 文件系统挂载 # 出现下面错误后： libbpf: specified path /sys/fs/bpf/dev_map is not on BPF FS libbpf: map 'dev_map': failed to auto-pin at '/sys/fs/bpf/dev_map': -22 libbpf: map 'dev_map': failed to create: Invalid argument(-22) libbpf: failed to load object 'cgroup_dev_bpf.o' Failed to load BPF object # 检查 bpf 文件系统是否挂载 mount | grep bpf # 挂载 BPF 文件系统 mkdir -p /sys/fs/bpf mount -t bpf bpf /sys/fs/bpf 2. 安装工具链 2.1 版本要求 工具链版本需符合以下要求： cmake版本>= 3.26 如果安装clang，版本>= 15 如果安装g++，版本>= 132.2 安装构建工具 dnf install -y \\ gcc-toolset-13 \\ cmake \\ patch \\ flex \\ bison \\ ninja-build echo 'source /opt/rh/gcc-toolset-13/enable' >> /etc/profile.d/extra.sh 2.3 安装常用工具 dnf install -y tar curl unzip git 此外还可以安装以下常用运维工具（可选）： dnf install -y tig tmux fish pdsh htop vim 3. 安装项目依赖 # 安装项目相关依赖包 dnf install -y \\ libstdc++-devel \\ libstdc++-static \\ openssl-devel \\ curl-devel \\ pam-devel \\ zlib-devel \\ libaio-devel \\ systemd-devel automake 安装后建议重新登陆集群 / 手动加载环境配置： source /etc/profile.d/extra.sh 4. 安装和配置 MongoDB 此步骤仅在存储节点（控制节点）进行，其他节点不需要安装数据库。 4.1 安装 MongoDB 1. 添加 MongoDB 的 YUM 源： cat > /etc/yum.repos.d/mongodb-org-7.0.repo 2. 安装 MongoDB 并添加开机启动 dnf install -y mongodb-org # 添加 MongoDB 开机启动 systemctl enable mongod systemctl start mongod 3. 使用 OpenSSL 生成密钥文件 openssl rand -base64 756 | sudo -u mongod tee /var/lib/mongo/mongo.key sudo -u mongod chmod 400 /var/lib/mongo/mongo.key 4.2 配置 MongoDB 1. 在数据库中创建用户 # 使用 mongosh 进入 MongoDB 命令行 mongosh # 进入 mongosh 后执行下列操作 use admin # user: 用户名 # pwd: 密码 # roles: root 代表超级管理员权限 # db: admin 代表给 admin 数据库添加超级管理员 db.createUser({ user: 'admin', pwd: '123456', roles: [{ role: 'root', db: 'admin' }] }) # 重启前先关闭服务器 db.shutdownServer() quit 2. 开启权限验证，并配置副本集 打开 /etc/mongod.conf 配置文件，找到 security 和 replication 部分，进行如下修改： vim /etc/mongod.conf # 以上省略... # 开启权限验证 security: authorization: enabled keyFile: /var/lib/mongo/mongo.key replication: # 副本集名称，稍后填写 crane 配置文件时需一致 replSetName: crane_rs 重启 MongoDB 使配置生效 systemctl restart mongod 3. 初始化副本集 mongosh use admin # 使用刚刚设置的账号密码登录 db.auth(\"admin\",\"123456\") # 如果不需要配置外部连接，并且副本集只有该主机一个节点，config 可不配置 config = { \"_id\": \"crane_rs\", # 注意名称一致 \"members\": [ { \"_id\": 0, \"host\": \":27017\" # 替换为部署副本集的主机名，默认为127.0.0.1 } # ... 其他节点（若有） ] # 初始化并启动副本集 rs.initiate() 5. 安装和配置 CraneSched 5.1 编译二进制文件 # 选择一个合适的位置 Clone 项目 git clone https://github.com/PKUHPC/CraneSched.git # 进入项目根目录 cd CraneSched # 创建并进入编译目录 mkdir -p build pushd build # 在编译目录下进行编译，首次编译需下载第三方库，耗时较长 cmake -G Ninja .. cmake --build . --target cranectld craned pam_crane # CgroupV2 cmake --build . --target cgroup_dev_bpf_object # 将可执行文件和 Systemd 服务安装到本机，其他节点需要手动拷贝 ninja install popd 注意：如果拉取仓库或依赖时出错，请使用代理 5.2 配置 PAM 模块 请在整个集群部署成功并正常运行后再进行此操作，否则会导致 SSH 认证失败无法连接！ 1. 将 PAM 模块拷贝到系统指定位置 # 在项目根目录下操作 cp build/src/Misc/Pam/pam_crane.so /usr/lib64/security/ 2. 修改 /etc/pam.d/sshd 配置文件 仿照以下样例配置文件（不同系统略有不同）， 找到加粗的行，在对应位置添加标红的行： 找到 account include password-auth，在之前添加 account required pam_crane.so 找到 session include password-auth，在之后添加 session required pam_crane.so #%PAM-1.0 auth substack password-auth auth include postlogin account required pam_sepermit.so account required pam_nologin.so account required pam_crane.so account include password-auth password include password-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open env_params session required pam_namespace.so session optional pam_keyinit.so force revoke session optional pam_motd.so session include password-auth session required pam_crane.so session include postlogin session optional pam_crane.so必须位于 session include password-auth之后。因为password-auth中有pam_systemd.so模块，会导致 sshd session 被移入systemd:/user.slice这个 cgroups 中。目前不清楚 systemd 是否会定期轮询相应的进程是否被 steal，待测试。 5.3 配置 CraneSched 1. 根据样例创建配置文件。样例配置文件在项目目录下etc/crane/ 中，将其拷贝到 /etc/crane 中： # 创建配置文件目录 mkdir -p /etc/crane # 假设当前在项目根目录 # 拷贝配置文件样例 cp etc/config.yaml /etc/crane/config.yaml cp etc/database.yaml /etc/crane/database.yaml # 如果使用 CGroup V2 cp build/src/Misc/BPF/cgroup_dev_bpf.o /etc/crane/cgroup_dev_bpf.o 2. 在 /etc/crane/config.yaml 中配置节点信息、调度偏好等选项（需所有节点保持一致）请根据集群实际情况填写。例如，一个四节点的集群中，控制节点的主机名为 crane01，计算节点的主机名为 crane01、crane02、crane03、crane04，则如下填写： vim /etc/crane/config.yaml # 以上省略... # 控制节点（主节点） ControlMachine: crane01 # ... # Nodes and partitions settings # 计算节点 Nodes: - name: \"crane[01-04]\" cpu: 2 memory: 2G # partition information list # 计算节点的分区 Partitions: - name: CPU # 分区的名称（可自定义） nodes: \"crane[01-02]\" # 分区中的节点，需要和 Nodes 部分对应 priority: 5 - name: GPU nodes: \"crane[03-04]\" priority: 3 # Optional default memory per cpu in MB DefaultMemPerCpu: 0 # 建议设置为0 # Optional maximum memory per cpu in MB, 0 indicates no limit MaxMemPerCpu: 0 # 建议设置为0 # 默认分区，未指定分区的作业将被提交到默认分区 DefaultPartition: CPU 3. 在 /etc/crane/database.yaml 中配置数据库信息（只需在控制节点配置） # EmbeddedDb settings # BerkeleyDB or Unqlite(default) CraneEmbeddedDbBackend: Unqlite # File path of CraneCtld embeded DB (Relative to CraneBaseDir) CraneCtldDbPath: cranectld/embedded.db # MongoDB 信息需要与数据库的配置相一致 DbUser: admin DbPassword: \"123456\" DbHost: localhost DbPort: 27017 DbReplSetName: crane_rs DbName: crane_db 6. 启动 CraneSched 可直接在前台启动 CraneSched（控制节点启动 CraneCtld，其他节点按需要启动 Craned） # 假设当前在项目根目录 cd build/src # 启动 Cranectld CraneCtld/cranectld # 启动 Craned Craned/craned 可通过 Systemd 在后台运行 CraneSched，并设置开机启动 # CraneCtld systemctl enable cranectld systemctl start cranectld # Craned systemctl enable craned systemctl start craned 附 1：常见问题 1. 如果运行 CMake 查找不到 libcgroup 包或版本不匹配，请参考“安装依赖”部分，安装 Release 版本的 libcgroup。 2. 运行 craned 时，系统无法找到 libcgroup.so.0 这个共享库。通常是因为该库不在系统的默认库搜索路径中。可使用 pkg-config 工具排查。 3. Craned 和 CraneCtld 都成功启动，但无法 cinfo 查询发现 Craned 没有上线。通常是因为没有关闭防火墙。 附 2：多节点环境部署说明 scp 在计算节点部署 Craned 时无需完整编译项目，仅需复制相应的可执行文件和配置文件： # 例如配置计算节点 crane02 ssh crane02 \"mkdir -p /etc/crane\" scp /usr/local/bin/craned crane02:/usr/local/bin/ scp /etc/systemd/system/craned.service crane02:/etc/systemd/system/ scp /etc/crane/config.yaml crane02:/etc/crane/ 注意：计算节点仍需完成“安装项目依赖”中的 libcgroup 安装部分。 PDSH 1. 更新 CraneCtld # 位于编译路径下执行 pdsh -w cranectl systemctl stop cranectld pdcp -w cranectl src/CraneCtld/cranectld /usr/local/bin pdsh -w cranectl systemctl start cranectld 2. 更新 Craned pdsh -w crane[01-04] systemctl stop craned pdcp -w crane[01-04] Craned/craned /usr/local/bin pdsh -w crane[01-04] systemctl start craned 附 3：便捷安装脚本 该脚本适用于 Rocky 9，包括工具链安装、项目依赖安装和 CraneSched 的编译 注意： 执行该脚本前需要先完成 环境准备 部分 执行该脚本后仍然需要自行 安装 MongoDB 和 配置 CraneSched 为保证依赖下载顺利，该脚本设置了代理，请在 setp 函数中修改代理配置 #!/bin/bash # Tested on Rocky Linux 9.3 set -eo pipefail # Function to set and unset proxy setp() { export https_proxy=http://crane:hf2lH9UUC3E0@192.168.1.1:7890 export http_proxy=http://crane:hf2lH9UUC3E0@192.168.1.1:7890 git config --global http.proxy $http_proxy git config --global https.proxy $https_proxy } unsetp() { unset http_proxy unset https_proxy git config --global --unset http.proxy git config --global --unset https.proxy } # Tools dnf install -y tar unzip git wget curl || { echo \"Error installing tools\" && exit 1 } # Dependency for libcgroup dnf install -y bison flex systemd-devel || { echo \"Error installing dependency\" && exit 1 } # Ensure the installation can be found export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH # Check if libcgroup is already installed if pkg-config --exists libcgroup; then echo \"libcgroup is already installed.\" else if [ ! -f \"libcgroup-3.1.0.tar.gz\" ]; then setp wget https://github.com/libcgroup/libcgroup/releases/download/v3.1.0/libcgroup-3.1.0.tar.gz || { echo \"Error downloading libcgroup\" && exit 1 } unsetp fi tar -xzf libcgroup-3.1.0.tar.gz && pushd libcgroup-3.1.0 (./configure --prefix=/usr/local && make -j && make install) || { echo \"Error compiling libcgroup\" && exit 1 } popd fi # Install dependencies and toolchain for Crane dnf install -y \\ patch \\ ninja-build \\ openssl-devel \\ pam-devel \\ zlib-devel \\ libatomic \\ libstdc++-static \\ libtsan \\ libasan \\ libaio \\ libaio-devel || { echo \"Error installing toolchain and dependency for craned\" && exit 1 } # libstdc++-static libatomic for debug # libtsan for CRANE_THREAD_SANITIZER # Check if cmake version is higher than 3.24 required_version=\"3.24\" install_version=\"3.28.1\" download_url=\"https://github.com/Kitware/CMake/releases/download/v${install_version}/cmake-${install_version}-linux-x86_64.sh\" current_version=$(cmake --version 2>/dev/null | awk 'NR==1{print $3}') if [[ -z \"$current_version\" ]] || [[ \"$(printf '%s\\n' \"$current_version\" \"$required_version\" | sort -V | head -n1)\" != \"$required_version\" ]]; then echo \"Installing cmake ${install_version}...\" setp wget -O cmake-install.sh \"$download_url\" || { echo \"Error downloading cmake\"; exit 1; } bash cmake-install.sh --skip-license --prefix=/usr/local || { echo \"Error installing cmake\"; exit 1; } rm cmake-install.sh unsetp else echo \"Current cmake version ($current_version) meets the requirement.\" fi # Clone the repository setp if [ ! -d \"CraneSched\" ]; then git clone https://github.com/PKUHPC/CraneSched.git || { echo \"Error cloning CraneSched\" && exit 1 } fi pushd CraneSched # git checkout master git fetch && git pull unsetp BUILD_DIR=cmake-build-release mkdir -p $BUILD_DIR && pushd $BUILD_DIR if [ -f \"/opt/rh/gcc-toolset-13/enable\" ]; then echo \"Enable gcc-toolset-13\" source /opt/rh/gcc-toolset-13/enable fi setp cmake --fresh -G Ninja \\ -DCMAKE_BUILD_TYPE=Release \\ -DENABLE_UNQLITE=ON \\ -DENABLE_BERKELEY_DB=OFF .. || { echo \"Error configuring with cmake\" && exit 1 } unsetp cmake --build . --clean-first || { echo \"Error building\" && exit 1 } popd popd Crane 前端环境配置-Rocky Linux 9 理论上在任何使用 systemd 的系统上都能使用（例如 Debian/Ubuntu/AlmaLinux/Fedora 等）。该教程涉及的软件基于 ARM64。如果使用 x86-64 等架构，请调整软件下载链接。请全程以 root 用户执行命令。建议先完成后端环境的安装。 1. 安装 Golang GOLANG_TARBALL=go1.22.0.linux-amd64.tar.gz curl -L https://go.dev/dl/${GOLANG_TARBALL} -o /tmp/go.tar.gz # 移除旧版本的 Golang 环境 rm -rf /usr/local/go tar -C /usr/local -xzf /tmp/go.tar.gz && rm /tmp/go.tar.gz echo 'export GOPATH=/root/go' >> /etc/profile.d/go.sh echo 'export PATH=$GOPATH/bin:/usr/local/go/bin:$PATH' >> /etc/profile.d/go.sh echo 'go env -w GO111MODULE=on' >> /etc/profile.d/go.sh echo 'go env -w GOPROXY=https://goproxy.cn,direct' >> /etc/profile.d/go.sh source /etc/profile.d/go.sh go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 2. 安装 Protoc PROTOC_ZIP=protoc-23.2-linux-x86_64.zip \\ curl -L https://github.com/protocolbuffers/protobuf/releases/download/v23.2/${PROTOC_ZIP} -o /tmp/protoc.zip unzip /tmp/protoc.zip -d /usr/local rm /tmp/protoc.zip /usr/local/readme.txt 3. 拉取项目 git clone https://github.com/PKUHPC/CraneSched-FrontEnd.git 4. 编译项目并部署前端 工作目录为CraneSched-FrontEnd，在该目录下编译所有 Golang 组件并安装。 cd CraneSched-FrontEnd make make install 5. 启动 Cfored 和 Cplugind（可选） 如果需要提交交互式任务（crun, calloc），则需要启用 Cfored： # 设置开机启动 systemctl enable cfored systemctl start cfored 如果配置文件中启用了插件系统，则需要启用 Cplugind： # 设置开机启动 systemctl enable cplugind systemctl start cplugind 6. 安装 Cwrapper 别名（可选） 可以通过下列命令安装 Crane 的 Slurm 别名，从而支持使用 Slurm 的命令形式使用 Crane： cat > /etc/profile.d/cwrapper.sh "},"config/Crane-ubuntu.html":{"url":"config/Crane-ubuntu.html","title":"3.3 Crane-ubuntu 环境配置","keywords":"","body":"Crane 后端环境配置 - Ubuntu 1. 环境准备 1.1 更新镜像源 最好将镜像源更换到国内 apt update && apt upgrade -y 1.2 安装证书 apt install ca-certificates -y 1.3 启用时间同步 apt install -y chrony systemctl restart systemd-timedated timedatectl set-timezone Asia/Shanghai timedatectl set-ntp true 1.4 关闭防火墙 systemctl stop ufw systemctl disable ufw 若集群不允许关闭防火墙，则考虑开放 10010、10011、10012、873 端口。 ufw allow 10012 ufw allow 10011 ufw allow 10010 ufw allow 873 注意：若有多个节点，需在每个节点上执行此步骤，否则无法进行节点间的通信。 2.安装项目依赖 apt install -y \\ libcgroup-dev \\ libssl-dev \\ libcurl-dev \\ libpam-dev \\ zlib1g-dev \\ libaio-dev \\ pkg-config \\ ninja \\ libelf-dev \\ bcc \\ linux-headers-$(uname -r) # 安装libbpf wget https://github.com/libbpf/libbpf/archive/refs/tags/v1.4.6.zip unzip v1.4.6.zip cd libbpf-1.4.6/src # 编译并安装 make make install 3. 安装工具链 工具链版本需符合以下要求： cmake版本>= 3.26 libstdc++版本>= 11 如果安装clang，版本>= 15 如果安装g++，版本>= 131. 安装常用工具 apt install -y wget tar unzip linux-tools-generic 2. 安装cmake Ubuntu 20.04 / 22.04: 从 GitHub 下载安装脚本 wget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4-linux-x86_64.sh bash cmake-3.26.4-linux-x86_64.sh --prefix=/usr/local --skip-license Ubuntu 24.04: 使用包管理器安装 apt install -y cmake 检查 cmake 安装情况 cmake --version # cmake version 3.26.4 3.安装新版 Clang wget https://apt.llvm.org/llvm.sh bash ./llvm.sh 19 4. 安装和配置 MongoDB 此步骤仅在存储节点（控制节点）进行，其他节点不需要安装数据库。 4.1 安装 MongoDB 安装 MongoDB 并添加开机启动 apt install -y mongodb-org # 添加开机启动 systemctl enable mongod systemctl start mongod 安装完mongod用户的home目录为/var/lib/mongo 利用openssl在/var/lib/mongo生成密钥文件 openssl rand -base64 756 | sudo -u mongod tee /var/lib/mongo/mongo.key sudo -u mongod chmod 400 /var/lib/mongo/mongo.key 4.2 配置 MongoDB 1. 在数据库中创建用户 # 使用 mongosh 进入 MongoDB 命令行 mongosh # 进入 mongosh 后执行下列操作 use admin # user: 用户名 # pwd: 密码 # roles: root 代表超级管理员权限 # db: admin 代表给 admin 数据库添加超级管理员 db.createUser({ user: 'admin', pwd: '123456', roles: [{ role: 'root', db: 'admin' }] }) # 重启前先关闭服务器 db.shutdownServer() quit 2. 开启权限验证，并配置副本集 打开 /etc/mongod.conf 配置文件，找到 security 和 replication 部分，进行如下修改： vim /etc/mongod.conf # 以上省略... # 开启权限验证 security: authorization: enabled keyFile: /var/lib/mongo/mongo.key replication: # 副本集名称，稍后填写 crane 配置文件时需一致 replSetName: crane_rs 重启 MongoDB 使配置生效 systemctl restart mongod 3. 初始化副本集 mongosh use admin # 使用刚刚设置的账号密码登录 db.auth(\"admin\",\"123456\") # 如果不需要配置外部连接，并且副本集只有该主机一个节点，config 可不配置 config = { \"_id\": \"crane_rs\", # 注意名称一致 \"members\": [ { \"_id\": 0, \"host\": \":27017\" # 替换为部署副本集的主机名，默认为127.0.0.1 } # ... 其他节点（若有） ] # 初始化并启动副本集 rs.initiate() 5. 安装和配置 CraneSched 编译Crane程序 #代理 git config --global http.proxy http://crane:hf2lH9UUC3E0@192.168.1.1:7890 git config --global https.proxy http://crane:hf2lH9UUC3E0@192.168.1.1:7890 # 选择一个合适的位置克隆项目 git clone https://github.com/PKUHPC/Crane.git cd Crane mkdir build cd build # 首次编译需要下载第三方库，耗时较长 cmake -G Ninja -DCMAKE_CXX_COMPILER=/usr/bin/clang++-17 -DCMAKE_C_COMPILER=/usr/bin/clang-17 .. cmake --build . --target cranectld craned pam_crane # 仅安装到本机，craned节点需手动scp ninja install 注意：如果拉取仓库或依赖时出错，请使用代理 6. 运行项目 首先根据自身的集群情况在配置文件当中进行相应配置，配置文件样例保存在etc/crane/config.yaml.example mkdir -p /etc/crane cp CraneSched安装路径/etc/config.yaml.example /etc/crane/config.yaml # 上述命令报错旧按照如下输入 cp CraneSched安装路径/etc/config.yaml /etc/crane/ cp CraneSched安装路径/build/src/Misc/BPF/cgroup_dev_bpf.o /etc/crane/cgroup_dev_bpf.o sudo cp etc/database.yaml /etc/crane/ 在 /etc/crane/config.yaml 中配置节点信息、调度偏好等选项（需所有节点保持一致）请根据集群实际情况填写。例如，一个四节点的集群中，控制节点的主机名为 crane01，计算节点的主机名为 crane01、crane02、crane03、crane04，则如下填写： vim /etc/crane/config.yaml # 以上省略... # 控制节点（主节点） ControlMachine: crane01 # ... # Nodes and partitions settings # 计算节点 Nodes: - name: \"crane[01-04]\" cpu: 2 memory: 2G # partition information list # 计算节点的分区 Partitions: - name: CPU # 分区的名称（可自定义） nodes: \"crane[01-02]\" # 分区中的节点，需要和 Nodes 部分对应 priority: 5 - name: GPU nodes: \"crane[03-04]\" priority: 3 # Optional default memory per cpu in MB DefaultMemPerCpu: 0 # 建议设置为0 # Optional maximum memory per cpu in MB, 0 indicates no limit MaxMemPerCpu: 0 # 建议设置为0 # 默认分区，未指定分区的作业将被提交到默认分区 DefaultPartition: CPU 3. 在 /etc/crane/database.yaml 中配置数据库信息（只需在控制节点配置） # EmbeddedDb settings # BerkeleyDB or Unqlite(default) CraneEmbeddedDbBackend: Unqlite # File path of CraneCtld embeded DB (Relative to CraneBaseDir) CraneCtldDbPath: cranectld/embedded.db # MongoDB 信息需要与数据库的配置相一致 DbUser: admin DbPassword: \"123456\" DbHost: localhost DbPort: 27017 DbReplSetName: crane_rs DbName: crane_db 6. 启动 CraneSched 可直接在前台启动 CraneSched（控制节点启动 CraneCtld，其他节点按需要启动 Craned） # 假设当前在项目根目录 cd build/src # 启动 Cranectld CraneCtld/cranectld # 启动 Craned Craned/craned 可通过 Systemd 在后台运行 CraneSched，并设置开机启动 # CraneCtld systemctl enable cranectld systemctl start cranectld # Craned systemctl enable craned systemctl start craned 7. 可能遇到的问题 7.1 libbpf error while loading shared libraries: libbpf.so.1: cannot open shared object file: No such file or directory # 验证安装是否正确 find /usr/lib -name \"libbpf*\" find /usr -name \"libbpf*\" # ubuntu 安装后.so文件可能会在/usr/lib64目录下，我们需要简历软链接 ln -s /usr/lib64/libbpf* /usr/lib/ 7.2 BPF 文件系统挂载 libbpf: specified path /sys/fs/bpf/dev_map is not on BPF FS libbpf: map 'dev_map': failed to auto-pin at '/sys/fs/bpf/dev_map': -22 libbpf: map 'dev_map': failed to create: Invalid argument(-22) libbpf: failed to load object 'cgroup_dev_bpf.o' Failed to load BPF object # 检查 bpf 文件系统是否挂载 mount | grep bpf # 挂载 BPF 文件系统 mkdir -p /sys/fs/bpf mount -t bpf bpf /sys/fs/bpf Crane 前端环境配置-Ubuntu 理论上在任何使用 systemd 的系统上都能使用（例如 Debian/Ubuntu/AlmaLinux/Fedora 等）。该教程涉及的软件基于 ARM64。如果使用 x86-64 等架构，请调整软件下载链接。请全程以 root 用户执行命令。建议先完成后端环境的安装。 1. 安装 Golang GOLANG_TARBALL=go1.22.0.linux-amd64.tar.gz curl -L https://go.dev/dl/${GOLANG_TARBALL} -o /tmp/go.tar.gz # 移除旧版本的 Golang 环境 rm -rf /usr/local/go tar -C /usr/local -xzf /tmp/go.tar.gz && rm /tmp/go.tar.gz echo 'export GOPATH=/root/go' >> /etc/profile.d/go.sh echo 'export PATH=$GOPATH/bin:/usr/local/go/bin:$PATH' >> /etc/profile.d/go.sh echo 'go env -w GO111MODULE=on' >> /etc/profile.d/go.sh echo 'go env -w GOPROXY=https://goproxy.cn,direct' >> /etc/profile.d/go.sh source /etc/profile.d/go.sh go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 2. 安装 Protoc PROTOC_ZIP=protoc-23.2-linux-x86_64.zip \\ curl -L https://github.com/protocolbuffers/protobuf/releases/download/v23.2/${PROTOC_ZIP} -o /tmp/protoc.zip unzip /tmp/protoc.zip -d /usr/local rm /tmp/protoc.zip /usr/local/readme.txt 3. 拉取项目 git clone https://github.com/PKUHPC/CraneSched-FrontEnd.git 4. 编译项目并部署前端 工作目录为CraneSched-FrontEnd，在该目录下编译所有 Golang 组件并安装。 cd CraneSched-FrontEnd make make install 5. 启动 Cfored 和 Cplugind（可选） 如果需要提交交互式任务（crun, calloc），则需要启用 Cfored： # 设置开机启动 systemctl enable cfored systemctl start cfored 如果配置文件中启用了插件系统，则需要启用 Cplugind： # 设置开机启动 systemctl enable cplugind systemctl start cplugind 6. 安装 Cwrapper 别名（可选） 可以通过下列命令安装 Crane 的 Slurm 别名，从而支持使用 Slurm 的命令形式使用 Crane： cat > /etc/profile.d/cwrapper.sh "},"config/Crane-OpenEuler22.html":{"url":"config/Crane-OpenEuler22.html","title":"3.4 Crane-OpenEuler 22 环境配置","keywords":"","body":"Crane 后端环境配置 - OpenEuler22 该教程基于 ARM64 架构。请全程以 root 用户执行命令 1. 环境准备 1.1 添加 EPEL 软件源 curl -o /etc/yum.repos.d/epel-OpenEuler.repo https://down.whsir.com/downloads/epel-OpenEuler.repo 1.2 启用时间同步 dnf install -y chrony systemctl restart systemd-timedated timedatectl set-timezone Asia/Shanghai timedatectl set-ntp true 1.3 关闭防火墙 systemctl stop firewalld systemctl disable firewalld 若集群不允许关闭防火墙，则考虑开放 10010、10011、10012、873 端口。 firewall-cmd --add-port=10012/tcp --permanent --zone=public firewall-cmd --add-port=10011/tcp --permanent --zone=public firewall-cmd --add-port=10010/tcp --permanent --zone=public firewall-cmd --add-port=873/tcp --permanent --zone=public firewall-cmd --reload 注意：若有多个节点，需在每个节点上执行此步骤，否则无法进行节点间的通信。 1.4 关闭SELinux #重启后失效 setenforce 0 #重启后生效 sed -i s#SELINUX=enforcing#SELINUX=disabled# /etc/selinux/config 1.5 切换CGroup 版本 mount | grep cgroup 如果显示 CgroupV2 需要切换为 V1 CraneSched 目前只支持 CGroup v1，需按照以下步骤切换版本 # 设置内核启动参数，更改 CGroup 版本。 grubby --update-kernel=/boot/vmlinuz-$(uname -r) --args=\"systemd.unified_cgroup_hierarchy=0 systemd.legacy_systemd_cgroup_controller\" # 重启系统生效 reboot # 验证版本 mount | grep cgroup 注意：所有计算节点（运行 Craned 的节点）均需要切换 CGroup 版本 2. 安装工具链 2.1 版本要求 工具链版本需符合以下要求： cmake版本>= 3.26 如果安装clang，版本>= 15 如果安装g++，版本>= 132.2 安装构建工具 dnf install -y \\ patch \\ flex \\ bison \\ ninja-build 下载和解压 GCC 源码 wget https://mirror.koddos.net/gcc/releases/gcc-13.2.0/gcc-13.2.0.tar.xz tar -xvf gcc-13.2.0.tar.xz cd gcc-13.2.0 下载依赖 ./contrib/download_prerequisites 创建编译目录并进入 mkdir -p build && cd build 配置编译参数，指定安装路径为 /opt/gcc-13.2.0 ../configure --prefix=/opt/gcc-13.2.0 --enable-languages=c,c++ 编译 GCC 并安装到 /opt/gcc-13.2.0 make -j$(nproc) make install 安装 cmake wget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4-linux-aarch64.sh bash cmake-3.26.4-linux-aarch64.sh --prefix=/usr/local --skip-license 检查 cmake 安装是否成功 cmake --version 2.3 安装常用工具 dnf install -y tar curl unzip git 此外还可以安装以下常用运维工具（可选）： dnf install -y tig tmux fish pdsh htop vim 3. 安装项目依赖 # 安装项目相关依赖包 dnf install -y \\ libstdc++-devel \\ libstdc++-static \\ openssl-devel \\ curl-devel \\ pam-devel \\ zlib-devel \\ libaio-devel \\ systemd-devel # 下载并编译安装 libcgroup curl -L https://github.com/libcgroup/libcgroup/releases/download/v3.1.0/libcgroup-3.1.0.tar.gz -o /tmp/libcgroup.tar.gz tar -C /tmp -xzf /tmp/libcgroup.tar.gz cd /tmp/libcgroup-3.1.0 ./configure --prefix=/usr/local make -j$(nproc) make install rm -rf /tmp/libcgroup-3.1.0 /tmp/libcgroup.tar.gz echo 'export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH' >> /etc/profile.d/extra.sh 安装后建议重新登陆集群 / 手动加载环境配置： source /etc/profile.d/extra.sh 4. 安装和配置 MongoDB 此步骤仅在存储节点（控制节点）进行，其他节点不需要安装数据库。 4.1 安装 MongoDB 1. 添加 MongoDB 的 YUM 源： cat > /etc/yum.repos.d/mongodb-org-7.0.repo 2. 安装 MongoDB 并添加开机启动 wget https://repo.mongodb.org/yum/redhat/8/mongodb-org/7.0/aarch64/RPMS/mongodb-org-database-tools-extra-7.0.9-1.el8.aarch64.rpm rpm -ivh --force ./mongodb-org-database-tools-extra-7.0.9-1.el8.aarch64.rpm --nodeps dnf install -y mongodb-org # 添加 MongoDB 开机启动 systemctl enable mongod systemctl start mongod 3. 使用 OpenSSL 生成密钥文件 openssl rand -base64 756 | sudo -u mongod tee /var/lib/mongo/mongo.key sudo -u mongod chmod 400 /var/lib/mongo/mongo.key 4.2 配置 MongoDB 1. 在数据库中创建用户 # 使用 mongosh 进入 MongoDB 命令行 mongosh # 进入 mongosh 后执行下列操作 use admin # user: 用户名 # pwd: 密码 # roles: root 代表超级管理员权限 # db: admin 代表给 admin 数据库添加超级管理员 db.createUser({ user: 'admin', pwd: '123456', roles: [{ role: 'root', db: 'admin' }] }) # 重启前先关闭服务器 db.shutdownServer() quit 2. 开启权限验证，并配置副本集 打开 /etc/mongod.conf 配置文件，找到 security 和 replication 部分，进行如下修改： vim /etc/mongod.conf # 以上省略... # 开启权限验证 security: authorization: enabled keyFile: /var/lib/mongo/mongo.key replication: # 副本集名称，稍后填写 crane 配置文件时需一致 replSetName: crane_rs 重启 MongoDB 使配置生效 systemctl restart mongod 3. 初始化副本集 mongosh use admin # 使用刚刚设置的账号密码登录 db.auth(\"admin\",\"123456\") # 如果不需要配置外部连接，并且副本集只有该主机一个节点，config 可不配置 config = { \"_id\": \"crane_rs\", # 注意名称一致 \"members\": [ { \"_id\": 0, \"host\": \":27017\" # 替换为部署副本集的主机名，默认为127.0.0.1 } # ... 其他节点（若有） ] # 初始化并启动副本集 rs.initiate() 5. 安装和配置 CraneSched 5.1 编译二进制文件 # 选择一个合适的位置 Clone 项目 git clone https://github.com/PKUHPC/CraneSched.git # 进入项目根目录 cd CraneSched # 创建并进入编译目录 mkdir -p build pushd build # 在编译目录下进行编译，首次编译需下载第三方库，耗时较长 cmake -G Ninja \\ -DCMAKE_CXX_COMPILER=/opt/gcc-13.2.0/bin/g++ \\ -DCMAKE_C_COMPILER=/opt/gcc-13.2.0/bin/gcc .. cmake --build . --target cranectld craned pam_crane # 将可执行文件和 Systemd 服务安装到本机，其他节点需要手动拷贝 ninja install popd 注意：如果拉取仓库或依赖时出错，请使用代理 5.2 配置 PAM 模块 请在整个集群部署成功并正常运行后再进行此操作，否则会导致 SSH 认证失败无法连接 1. 将 PAM 模块拷贝到系统指定位置 # 在项目根目录下操作 cp build/src/Misc/Pam/pam_crane.so /usr/lib64/security/ 2. 修改 /etc/pam.d/sshd 配置文件 仿照以下样例配置文件（不同系统略有不同）， 找到加粗的行，在对应位置添加标红的行： 找到 account include password-auth，在之前添加 account required pam_crane.so 找到 session include password-auth，在之后添加 session required pam_crane.so #%PAM-1.0 auth substack password-auth auth include postlogin account required pam_sepermit.so account required pam_nologin.so account required pam_crane.so account include password-auth password include password-auth # pam_selinux.so close should be the first session rule session required pam_selinux.so close session required pam_loginuid.so # pam_selinux.so open should only be followed by sessions to be executed in the user context session required pam_selinux.so open env_params session required pam_namespace.so session optional pam_keyinit.so force revoke session optional pam_motd.so session include password-auth session required pam_crane.so session include postlogin session optional pam_crane.so必须位于 session include password-auth之后。因为password-auth中有pam_systemd.so模块，会导致 sshd session 被移入systemd:/user.slice这个 cgroups 中。目前不清楚 systemd 是否会定期轮询相应的进程是否被 steal，待测试。 5.3 配置 CraneSched 1. 根据样例创建配置文件。样例配置文件在项目目录下etc/crane/ 中，将其拷贝到 /etc/crane 中： # 创建配置文件目录 mkdir -p /etc/crane # 假设当前在项目根目录 # 拷贝配置文件样例 cp etc/config.yaml /etc/crane/config.yaml cp etc/database.yaml /etc/crane/database.yaml 2. 在 /etc/crane/config.yaml 中配置节点信息、调度偏好等选项（需所有节点保持一致）请根据集群实际情况填写。例如，一个四节点的集群中，控制节点的主机名为 crane01，计算节点的主机名为 crane01、crane02、crane03、crane04，则如下填写： vim /etc/crane/config.yaml # 以上省略... # 控制节点（主节点） ControlMachine: crane01 # ... # Nodes and partitions settings # 计算节点 Nodes: - name: \"crane[01-04]\" cpu: 2 memory: 2G # partition information list # 计算节点的分区 Partitions: - name: CPU # 分区的名称（可自定义） nodes: \"crane[01-02]\" # 分区中的节点，需要和 Nodes 部分对应 priority: 5 - name: GPU nodes: \"crane[03-04]\" priority: 3 # Optional default memory per cpu in MB DefaultMemPerCpu: 0 # 建议设置为0 # Optional maximum memory per cpu in MB, 0 indicates no limit MaxMemPerCpu: 0 # 建议设置为0 # 默认分区，未指定分区的作业将被提交到默认分区 DefaultPartition: CPU 3. 在 /etc/crane/database.yaml 中配置数据库信息（只需在控制节点配置） # EmbeddedDb settings # BerkeleyDB or Unqlite(default) CraneEmbeddedDbBackend: Unqlite # File path of CraneCtld embeded DB (Relative to CraneBaseDir) CraneCtldDbPath: cranectld/embedded.db # MongoDB 信息需要与数据库的配置相一致 DbUser: admin DbPassword: \"123456\" DbHost: localhost DbPort: 27017 DbReplSetName: crane_rs DbName: crane_db 6. 启动 CraneSched 可直接在前台启动 CraneSched（控制节点启动 CraneCtld，其他节点按需要启动 Craned） # 假设当前在项目根目录 cd build/src # 启动 Cranectld CraneCtld/cranectld # 启动 Craned Craned/craned 可通过 Systemd 在后台运行 CraneSched，并设置开机启动 # CraneCtld systemctl enable cranectld systemctl start cranectld # Craned systemctl enable craned systemctl start craned 附 1：常见问题 1. 如果运行 CMake 查找不到 libcgroup 包或版本不匹配，请参考“安装依赖”部分，安装 Release 版本的 libcgroup。 2. 运行 craned 时，系统无法找到 libcgroup.so.0 这个共享库。通常是因为该库不在系统的默认库搜索路径中。可使用 pkg-config 工具排查。 3. Craned 和 CraneCtld 都成功启动，但无法 cinfo 查询发现 Craned 没有上线。通常是因为没有关闭防火墙。 附 2：多节点环境部署说明 scp 在计算节点部署 Craned 时无需完整编译项目，仅需复制相应的可执行文件和配置文件： # 例如配置计算节点 crane02 ssh crane02 \"mkdir -p /etc/crane\" scp /usr/local/bin/craned crane02:/usr/local/bin/ scp /etc/systemd/system/craned.service crane02:/etc/systemd/system/ scp /etc/crane/config.yaml crane02:/etc/crane/ 注意：计算节点仍需完成“安装项目依赖”中的 libcgroup 安装部分。 PDSH 1. 更新 CraneCtld # 位于编译路径下执行 pdsh -w cranectl systemctl stop cranectld pdcp -w cranectl src/CraneCtld/cranectld /usr/local/bin pdsh -w cranectl systemctl start cranectld 2. 更新 Craned pdsh -w crane[01-04] systemctl stop craned pdcp -w crane[01-04] Craned/craned /usr/local/bin pdsh -w crane[01-04] systemctl start craned Crane 前端环境配置-OpenEuler22 理论上在任何使用 systemd 的系统上都能使用（例如 Debian/Ubuntu/AlmaLinux/Fedora 等）。该教程涉及的软件基于 ARM64。如果使用 x86-64 等架构，请调整软件下载链接。请全程以 root 用户执行命令。建议先完成后端环境的安装。 1. 安装 Golang GOLANG_TARBALL=go1.22.0.linux-amd64.tar.gz curl -L https://go.dev/dl/${GOLANG_TARBALL} -o /tmp/go.tar.gz # 移除旧版本的 Golang 环境 rm -rf /usr/local/go tar -C /usr/local -xzf /tmp/go.tar.gz && rm /tmp/go.tar.gz echo 'export GOPATH=/root/go' >> /etc/profile.d/go.sh echo 'export PATH=$GOPATH/bin:/usr/local/go/bin:$PATH' >> /etc/profile.d/go.sh echo 'go env -w GO111MODULE=on' >> /etc/profile.d/go.sh echo 'go env -w GOPROXY=https://goproxy.cn,direct' >> /etc/profile.d/go.sh source /etc/profile.d/go.sh go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 2. 安装 Protoc PROTOC_ZIP=protoc-23.2-linux-x86_64.zip \\ curl -L https://github.com/protocolbuffers/protobuf/releases/download/v23.2/${PROTOC_ZIP} -o /tmp/protoc.zip unzip /tmp/protoc.zip -d /usr/local rm /tmp/protoc.zip /usr/local/readme.txt 3. 拉取项目 git clone https://github.com/PKUHPC/CraneSched-FrontEnd.git 4. 编译项目并部署前端 工作目录为CraneSched-FrontEnd，在该目录下编译所有 Golang 组件并安装。 cd CraneSched-FrontEnd make make install 5. 启动 Cfored 和 Cplugind（可选） 如果需要提交交互式任务（crun, calloc），则需要启用 Cfored： # 设置开机启动 systemctl enable cfored systemctl start cfored 如果配置文件中启用了插件系统，则需要启用 Cplugind： # 设置开机启动 systemctl enable cplugind systemctl start cplugind 6. 安装 Cwrapper 别名（可选） 可以通过下列命令安装 Crane 的 Slurm 别名，从而支持使用 Slurm 的命令形式使用 Crane： cat > /etc/profile.d/cwrapper.sh "},"config/CraneSched.html":{"url":"config/CraneSched.html","title":"3.5 CraneSched配置文件","keywords":"","body":"CraneSched 配置文件 vim /etc/crane/config.yaml # 以上省略... # 控制节点（主节点） ControlMachine: crane01 # ... # Nodes and partitions settings # 计算节点 Nodes: - name: \"crane[01-04]\" #每个节点的资源 cpu: 2 memory: 2G #设备资源 gres: #资源的name - name: gpu #资源的type type: a100 # 设备资源对应的/dev设备文件，每个文件解析为一个设备 DeviceFileRegex: /dev/nvidia[0-3] # 设备资源对应的/dev设备文件，/dev/dri/renderer[0-3]解析为一个设备 DeviceFileList: - /dev/dri/renderer[0-3] - /dev/dri/renderer[4-7] #设备对应的环境变量 如CUDA_VISIABLE_DEVICES等 #可选：nvidia,hip,ascend EnvInjector: nvidia # partition information list # 计算节点的分区 Partitions: - name: CPU # 分区的名称（可自定义） nodes: \"crane[01-02]\" # 分区中的节点，需要和 Nodes 部分对应 priority: 5 - name: GPU nodes: \"crane[03-04]\" priority: 3 # Optional default memory per cpu in MB, 0 let scheduler decide DefaultMemPerCpu: 0 # 建议设置为0 # Optional maximum memory per cpu in MB, 0 indicates no limit MaxMemPerCpu: 0 # 建议设置为0 # 默认分区，未指定分区的作业将被提交到默认分区，需要和 Partitions 部分对应 DefaultPartition: CPU Gres配置 设备资源相关配置 name：一般是资源类型如：GPU，NPU等 type：一般是资源型号如：A100，3090等 DeviceFileRegex: 资源对应的 /dev 目录下的设备文件，适用于一个物理设备对应一个设备文件的资源，每个文件对应系统内的一个 Gres 资源，支持 Regex 格式。常见设备对应设备文件。如 Nvidia、AMD、海光 DCU、昇腾等。 DeviceFileList：适用于一个物理设备对应多个 /dev 目录下设备文件的 Gres 资源，每一组文件对应系统内的一个 Gres 资源，支持 Regex 格式。DeviceFileRegex与DeviceFileList二选一，以上设备文件必须存在，否则 Craned 启动时将报错退出 EnvInjector: 设备需要注入的环境变量可选值：对应环境变量 nvidia：CUDA_VISIABLE_DEVICES hip：HIP_VISIABLE_DEVICES ascend：ASCEND_RT_VISIBLE_DEVICES常见厂商设备文件路径及相关配置 厂商 设备文件路径 EnvInjector Nvidia dev/nvidia0 ... nvidia AMD/海光DCU /dev/dri/renderer128... hip 昇腾 /dev/davinci0 ... ascend "},"command/":{"url":"command/","title":"4 CraneSched使用教程","keywords":"","body":"CraneSched使用教程 slurm是目前高性能计算调度领域使用最为广泛的调度系统，用户众多，为了减少用户使用鹤思智能调度系统的学习成本，鹤思在前端用户操作指令的设计时，主要借鉴Slurm的指令原型，便于用户快速从Slurm系统切换到鹤思调度系统。此外，鹤思在用户指令的使用提示方面进行了全方位的体验提升，用户可按照系统的帮助提示一步步完成操作。目前鹤思智能调度系统已经完成查看节点与分区状态、提交批处理作业、查看未结束的作业、取消作业、查看分区/节点状态、管理QoS/用户/账户信息、查看所有已结束/未结束的作业信息等功能的研发。 CraneSched有以下9个常用指令，请点击查看更详细使用信息 cinfo： 查看节点与分区状态 cbatch： 提交批量处理作业 crun： 提交交互式任务 calloc： 提交交互式任务 cqueue： 查看作业队列 ccancel： 取消运行或提交的作业 ccontrol：查看/修改分区和节点状态 cacctmgr： 查看和调整用户/账号信息 cacct： 查看作业信息 "},"command/cinfo.html":{"url":"command/cinfo.html","title":"4.1 cinfo 查看节点与分区状态","keywords":"","body":"cinfo 查看节点与分区状态 cinfo可查询各分区节点的队列资源信息。 查看分区节点状态： cinfo cinfo运行结果展示 主要输出项 PARTITION：分区名 AVAIL： 分区状态 UP： 可用 DOWN： 不可用 NODES：节点数 STATE：节点状态 idel： 空闲 mix： 节点部分核心可以使用 alloc： 节点已被占用 down： 节点不可用 NODELIST： 节点列表 主要参数 -h/--help：显示帮助 -C/--config string：配置文件路径(默认为 \"/etc/crane/config.yaml\") -d/--dead：只显示无响应节点 -i/--iterate uint：指定间隔秒数刷新查询结果。如 -i=3 表示每隔三秒输出一次查询结果 --json：json格式输出命令执行结果 -n/--nodes string：显示指定节点信息，多个节点用逗号隔开。例：cinfo -n crane01,crane02 -N/--noheader:输出隐藏表头 -p/--partition string：显示指定分区信息，多个分区用逗号隔开。例：cinfo -p CPU,GPU -r/--responding：只显示有响应节点 -t/--states string：仅显示状态的信息。状态可以为(不区分大小写): IDLE, MIX, ALLOC和DOWN -v/--verison: 查阅版本号 例cinfo cinfo -h cinfo -N cinfo -d cinfo -i 3 cinfo -n crane01,crane02.crane03 cinfo -p GPU,CPU cinfo -r cinfo -t IDLE cinfo -V "},"command/cbatch.html":{"url":"command/cbatch.html","title":"4.2 cbatch 提交批处理作业","keywords":"","body":"cbatch 提交批处理作业 cbatch主要是将用户描述整个计算过程的脚本传递给作业调度系统，并为作业分配作业号，等待作业调度系统为其分配资源并执行。 CraneSched系统中必须有用户和账号才能提交作业，添加用户和账户请参考cacctmgr教程。 首先介绍一个简单的单节点作业的例子: 下列作业将申请一个节点，一个CPU核心，并在计算节点上运行hostname并退出 #!/bin/bash #CBATCH --ntasks-per-node 1 #CBATCH --node 1 #CBATCH -c 1 #CBATCH --mem 20M #CBATCH --time 0:3:1 #CBATCH -o job.out #CBATCH -p CPU #CBATCH -J Test_Job hostname 假设上面作业脚本的文件名为cbatch_test.sh，通过cbatch命令提交： cbatch cbatch_test.sh cbatch运行结果展示 主要参数： --help/-h：显示帮助 -A/--account string：提交作业的账户 -D/--chdir string:任务工作路径 -C/--config string：配置文件路径(默认 \"/etc/crane/config.yaml\") -c/--cpus-per-task float：每个节点申请的CPU核心数 -e/--error string: 指定脚本错误日志定向路径 -J/--job-name string：作业名 --mem string：每个节点申请的内存大小 -N/--nodes uint32：申请的节点数量 --ntasks-per-node uint32：每个节点上运行的任务数量 -o/--output string：指定作业的标准输出重定向 -p/--partition string：作业使用的分区/队列 -q/--qos string：指定作业使用的qos名称 -t/--time string：作业的最长运行时间 --repeat uint32：以指定的重复次数提交作业，默认为1 -w/--nodelist：提交作业到指定节点运行 -x/--exclude：提交的作业排除某些指定节点运行 --get-user-env：获取用户的环境变量 --gres:任务申请的设备资源量 格式为name:type:count如GPU:A100:2 或者name:count如GPU:2，由调度器决定分配给任务的设备type --export string：设置环境变量 --extra-attr string: json格式提交额外参数 --json: json格式输出命令执行结果 _ -v/--version: 查询版本号 --mail-type, --mail-user：设置邮件提醒功能 --mail-user：设置邮件提醒的收件地址 --mail-type：设置邮件提醒在任务运行的哪些阶段发送（可用的值包括：NONE、BEGIN、END、FAIL、ALL，默认为 None 即不发送提醒） 注意：使用邮件提醒功能必须预先配置 Linux 的 mail 命令，同时在 CraneSched 后端启用邮件功能，否则邮件相关参数将被忽略。 例如： cbatch --mail-type=ALL --mail-user=foo@bar.com cbatch_test.sh cbatch cbatch_test.sh cbatch -h cbatch -A=acct-test cbatch_test.sh cbatch -J testjob01 cbatch_test.sh cbatch -w crane01,crane03 cbatch_test.sh cbatch -p GPU cbatch_test.sh cbatch -t 00:25:25 cbatch_test.sh cbatch -c 2 cbatch_test.sh cbatch --mem 123M cbatch_test.sh cbatch -N 2 --ntasks-per-node 2 cbatch_test.sh cbatch -D /path test.sh cbatch -e error.log test.sh cbatch --export ALL test.sh cbatch --get-user-env test.sh cbatch -o output.out test.sh cbatch -q qos_test test.sh cbatch --repeat 3 test.sh 常用环境变量 变量名 说明 CRANE_JOB_NODELIST 作业分配的节点列表 %j 作业号 下面介绍提交一个跨节点多核心的例子： 下列作业将在三个节点上运行，每个节点使用4个CPU核心。 #!/bin/bash #CBATCH -o crane_test%j.out #CBATCH -p CPU #CBATCH -J \"crane_test\" #CBATCH --node 3 ##CBATCH --ntasks-per-node 4 #CBATCH -c 4 #CBATCH --time 50:00:00 # 生成作业分配的节点的machinefile echo \"$CRANE_JOB_NODELIST\" | tr \";\" \"\\n\" > crane.hosts #加载MPI运行环境 module load mpich/4.0 #执行跨节点并行任务 mpirun -n 13 -machinefile crane.hosts helloWorld > log "},"command/crun.html":{"url":"command/crun.html","title":"4.3 crun 提交交互式任务","keywords":"","body":"crun 提交交互式任务 crun使用命令行指定的参数申请资源并在计算节点启动指定的任务，用户的输入将被转发到计算节点上对应的任务，任务的输出将被转发回用户终端。crun需要在有cfored运行的节点上启动。 crun只支持通过命令行指定请求参数，支持的命令行选项： --help/-h：显示帮助 -A/--account string：提交作业的账户 -D/--chdir string:任务工作路径 -C/--config string：配置文件路径(默认 \"/etc/crane/config.yaml\") -c/--cpus-per-task float：每个节点申请的CPU核心数 -J/--job-name string：作业名 --mem string：每个节点申请的内存大小，不设置或者为0将使用调度器默认内存 -N/--nodes uint32：申请的节点数量 --ntasks-per-node uint32：每个节点上运行的任务数量 -p/--partition string：作业使用的分区/队列 -q/--qos string：指定作业使用的qos名称 -t/--time string：作业的最长运行时间 -w/--nodelist：提交作业到指定节点运行 -x/--exclude string：提交的作业排除某些指定节点运行 --export string：设置环境变量 --get-user-env：获取用户的环境变量 --json: json格式输出命令执行结果 -v/--version: 查询版本号 --debug-level string: 日志输出等级 --gres： 任务申请的设备资源量 格式为name:type:count如GPU:A100:2 或者name:count如GPU:2，由调度器决定分配给任务的设备type 在一秒内两次ctrl+c发送sigint信号、使用ccancel取消任务、或者节点上的任务进程退出会结束任务。 在CPU分区，申请两个节点，一个CPU核心，200M内存，并运行bash程序： crun -c 1 --mem 200M -p CPU -N 2 /usr/bin/bash 运行结果： 例：申请一个节点，且节点不能是crane01,crane02，任务名称为testjob，运行时间限制为0:25:25，并运行bash程序： 例：在GPU分区申请一个节点和200M运行内存，节点只能在crane02、crane03中选择，并运行bash程序： crun还可以在calloc任务内嵌套启动，将自动继承calloc任务的所有资源。不需要指定除需要运行的程序外其他参数。 crun -A ROOT -J test_crun -x cranetest03 --get-user-env --ntasks-per-node 2 -q test_qos -t 00:20:00 /usr/bin/bash crun -D --debug-level trace --export ALL /path /usr/bin/bash crun -w cranetest04 /usr/bin/bash "},"command/calloc.html":{"url":"command/calloc.html","title":"4.4 calloc 提交交互式任务","keywords":"","body":"calloc 提交交互式任务 calloc 使用命令行指定的参数申请资源，任务启动时，会进入新的用户终端，用户需要自行登陆到计算节点并启动任务。calloc需要在有cfored运行的节点上启动。 calloc 只支持通过命令行指定请求参数，支持的命令行选项： --help/-h：显示帮助 -A/--account string：提交作业的账户 -D/--chdir string：任务工作路径 -C/--config string：配置文件路径(默认 \"/etc/crane/config.yaml\") -c/--cpus-per-task float：每个节点申请的CPU核心数 -J/--job-name string：作业名 --mem string：每个节点申请的内存大小 -N/--nodes uint32：申请的节点数量 --ntasks-per-node uint32：每个节点上运行的任务数量 -p/--partition string：作业使用的分区/队列 -q/--qos string：指定作业使用的qos名称- -t/--time string：作业的最长运行时间 -w/--nodelist string：提交作业到指定节点运行 -x/--exclude string：提交的作业排除某些指定节点运行 --export string：设置环境变量 --get-user-env：获取用户的环境变量 --gres： 任务申请的设备资源量 格式为name:type:count如GPU:A100:2 或者name:count如GPU:2，由调度器决定分配给任务的设备type -v/--version: 查询版本号 --debug-level string: 日志输出等级 --json: json格式输出命令执行结果 calloc -h 退出calloc新启动的终端将结束任务。 在CPU分区，申请两个个节点，一个CPU核心，200M内存 calloc -c 1 --mem 200M -p CPU -N 2 运行结果： 例：在GPU分区下，申请一个节点，每个节点上运行两个任务，申请节点的候选列表为crane02,crane03，且任务提交在acct-yan账户下 calloc -A acct-test --ntasks-per-node 2 -w crane02,crane03 -p GPU -N 1 例：在CPU分区下，申请200M内存，任务运行最长时间为25分钟25秒，且任务运行在test-qos下 calloc --mem 200M -p CPU -q test-qos -t 00:25:25 calloc -D /path calloc --debug-level trace calloc -x cranetest02 calloc --get-user-env calloc -J job_name "},"command/cqueue.html":{"url":"command/cqueue.html","title":"4.5 cqueue 查看作业队列","keywords":"","body":"cqueue 查看作业队列 cqueue可以查看队列中的作业信息。 查看集群中所有队列的作业信息（包括状态pending、running、cancelled），默认输出100条信息。 cqueue cqueue运行结果展示 主要输出项 JobId：作业号 Partition：作业所在分区 Name：作业名 User：作业所属用户 Account：作业所属账户 Status：作业状态 Type： 作业类型 TimeLimit：作业时间限制 Nodes：作业所分配节点数 NodeList：作业运行的节点名 主要参数 -A/--Account string：指定查询作业所属账户，指定多个账户时用逗号隔开 -C/--config string：配置文件路径 -o/--format string: 以指定格式输出结果表，可以指定输出指定列以及列宽 -F/--full: 显示完整的内容，如果未指定，默认每项输出30个字符 -h/--help: 显示帮助 -i/--iterate uint：指定间隔秒数刷新查询结果。如 -i=3表示每隔三秒输出一次查询结果 -j/--job string: 指定查询作业号，指定多个作业号时用逗号隔开。如 -j=2,3,4 --json: json格式输出命令执行结果 -m/--MaxVisibleLines uint32：指定输出结果的最大条数。如-m=500表示最多输出500行查询结果 -n/--name string：指定查询作业名，指定多个作业名时用逗号隔开 -N/--noHeader：输出隐藏表头 -p/--partition string：指定查询作业所在分区，指定多个分区时用逗号隔开 -q/--qos string：指定查询作业的QoS，指定多个QoS时用逗号隔开 --self：查看当前用户提交的作业 -S/--start：显示作业的开始时间（pending作业显示预期开始时间） -t/--state string：指定查询作业状态，指定多个状态时用逗号隔开 -u/--user：指定查询作业所属用户，指定多个用户时用逗号隔开 /--version：查询版本号 例cqueue -h cqueue -N cqueue -S cqueue -j 30674,30675 cqueue -t Pending cqueue -t r cqueue -u cranetest cqueue -A CraneTest cqueue -i 3 cqueue -p CPU cqueue -m 3 cqueue -o=\"%n %u %.5j %.5t %.3T %.5T\" format中的指定列的对应缩写对照： j-TaskId；n-Name；t-State；p-Partition；u-User；a-Account；T-Type；I-NodeIndex；l-TimeLimit；N-Nodescqueue -n test cqueue -N cqueue -q test_qos cqueue --self cqueue -t Running -S 2024-01-02T15:04:05~2024-01-11T11:12:41 "},"command/ccancel.html":{"url":"command/ccancel.html","title":"4.6 ccancel 取消运行或提交的作业","keywords":"","body":"ccancel 取消作业 ccancel可以终止正在运行或者在排队中的作业。 主要参数 -h/--help: 显示帮助 -A/--account string：取消账户下的任务 -C/--config string：配置文件路径(默认 \"/etc/crane/config.yaml\") -n/--name string：仅使用任务名称取消任务 -w/--nodes strings：取消节点上运行的任务 -p/--partition string：取消分区上运行的任务 -t/--state string：取消某状态的任务。有效的任务状态是 PENDING(PD)、RUNNING(R)。任务状态不区分大小写 -u/--user string：取消特定用户提交的任务 --json：json格式输出命令执行结果 -v/--version：查询版本号 例 ccancel -w crane02 ccancel -t Pending 取消作业号为30686的作业： ccancel 30686 ccancel运行结果展示 取消作业号为test1的作业： ccancel -n test ccancel运行结果展示 取消CPU分区上的作业 ccancel -p GPU ccancel运行结果展示 ccancel -A PKU ccancel -u ROOT 取消作业之后，如果被分配节点上没有用户的其他作业，作业调度系统会终止用户在所分配节点的所有进程，并取消用户在所分配节点上的ssh权限。 "},"command/ccontrol.html":{"url":"command/ccontrol.html","title":"4.7 ccontrol 查看/修改分区和节点状态","keywords":"","body":"ccontrol 查看/修改分区和节点状态 ccontrol可以查看/修改分区和节点的状态。 主要命令 help：显示帮助 show: 显示实体的状态，默认为所有记录 add：添加一个分区或者节点 update：修改作业/分区/节点信息 hold：暂停作业调度 release：继续作业调度支持的命令行选项 -h/--help: 显示帮助 --json：json格式输出命令执行结果 -v/--version：查询版本号 -C/--config string：配置文件路径(默认 \"/etc/crane/config.yaml\") 查看 支持的命令选项 config: 查询配置信息 job：查询作业信息 node：查询节点信息 partition：查看集群分区情况ccontrol show -h 1. 查看分区状态 ccontrol show partition ccontrol show partition运行结果展示 主要输出项 PartitionName：分区名 State：分区状态 TotalNodes：分区节点数目 AliveNodes：分区中可运行的节点数目 TotalCpus：分区中所有节点总CPU数目 AvailCpus：分区中所有可以使用的CPU数目 AllocCpus：分区中已经被分配的CPU数目 FreeCpus：分区中空闲的CPU数目 TotalMem：分区节点的总内存 AvailMem：分区中当前可以使用的内存大小 AllocMem：分区中已分配的内存大小 FreeMem：分区中空闲的内存大小 HostList：分区中所有节点的节点名列表 2. 查看节点状态 ccontrol show node ccontrol show node运行结果展示 主要输出项 NodeName：节点名 State：节点状态 IDLE：节点空闲，可使用 DOWN：节点宕机，不可用 CPUs：节点CPU数目 AllocCpus：节点已分配的CPU数目 FreeCpus：节点空闲的CPU数目 RealMemory：节点的实际内存大小 AllocMem：节点已经分配的内存大小 FreeMem：节点空闲的内存大小 Patition：节点所属分区 RunningTask：节点上正在运行的作业数量 3. 查看作业状态 ccontrol show job ccontrol show job 运行结果展示 主要输出项 JobId：作业号 JobName：作业名 UserId：作业所属用户 GroupId：分组id Account：作业所属账户 JobState：作业状态 RunTime：作业运行时间 TimeLimit：作业运行时间限制 SubmitTime：作业提交时间 StartTime：作业开始时间 EndTime：作业结束时间 Partition：作业所属分区 Nodelist：作业运行的节点 NumNodes：节点数量 2.修改 支持的命令行选项 job：查询作业信息 node：查询节点信息ccontrol update -h 1. 修改作业信息 支持的命令行选项 -h/--help: 显示帮助 -J/--job-name string：作业名 -P/--priority float： 作业优先级 -T/--time-limit string：作业超时时长ccontrol update job -h ccontrol update job -J 30685 -T 0:25:25 ccontrol update job -J 191 -P 2.0 2.修改节点信息 支持的命令行选项 -h/--help: 显示帮助 -n/--name string：节点名 -r/--reason string： 设置修改原因 -t/--state string：修改节点状态ccontrol update node -h ccontrol update node -n crane01 -t drain -r improving performance 主要参数 -c/--cpu：节点的核心数（-h列表无该参数） -M/--memory：节点的内存大小，默认是MB（-h列表无该参数） -n/--name：节点名称 -P/--partition：节点所属的分区（-h列表无该参数） 以下参数和上面参数不能一起设置，下面参数用于修改节点状态 -r/--reason：设置状态改变原因 -t/--state：设置节点状态 3. 暂停/恢复 3.1. 暂停作业调度 主要参数 --time-limit/-T：修改时间限制 ccontrol hold 30751 #暂停调度编号为30751的任务 ccontrol hold 30751 -t 0:25:25 #暂停调度编号为30751的任务25分钟25秒钟（随后解除暂停） hold 接受 job_id 的方式与 ccancel 相同，要求为逗号分隔的任务编号。 只能 hold pending 任务。 如果此前有设置解除暂停的定时器，该操作会取消原有的定时器。 使用 cqueue 查询时，如果任务被 hold，Node(Reason) 一列会显示 \"Held\"。 3.2. 继续作业调度 ccontrol release 30751 如果此前有设置解除暂停的定时器，该操作会取消原有的定时器。 只能 release pending 任务1 completion 主要命令 bash：为bash生成自动补全脚本 fish：为fish生成自动补全脚本 powershell：为powershell生成自动补全脚本 zsh：为zsh生成自动补全脚本 "},"command/cacctmgr.html":{"url":"command/cacctmgr.html","title":"4.8 cacctmgr 查看和调整用户/账号信息","keywords":"","body":"cacctmgr 管理用户/账户信息 cacctmgr 可以管理账户/用户信息，包括添加账户/用户、删除账户/用户、查找账户/用户。 Crane作业调度系统中有三个用户角色： 系统管理员（Admin）：一般为root用户，可以增删查改任何账户和用户信息 平台管理员（Operator）：对账户系统具有完全权限 账户调度员（Coordinator）：对与自身同一账户下的用户以及对自身账户的子账户具有操作权限，包括添加用户 普通用户(None)：除了查询功能外不具备其他权限，能够查询与自身同一账户下的信息，不可以修改所有用户和账户信息 主要参数 -h/--help：显示帮助 -C, --config string：配置文件路径（默认为\"/etc/crane/config.yaml\"） 主要命令 help：显示帮助 add：添加实体（实体包括QoS、账户、用户） block：禁用该实体，使其无法使用 delete：删除实体 modify：修改实体 show：显示一类实体的所有记录 unblock：解除禁用 1. 添加 1.1 添加qos 主要参数 -D/--description string：qos描述信息 -h/--help：帮助 -c/--max_cpus_per_user uint32：默认为10 -J/--max_jobs_per_user uint32 -T/--max_time_limit_per_task uint：以秒为单位的时间（默认3600） -N/--name string：qos的名称 -P/--priority uint32：默认为1000 例 cacctmgr add qos -N=test -D=\"test qos\" 1.2 添加账户 主要参数 -Q, --default_qos string：账户默认qos -D, --description string：账号描述信息 -h, --help：帮助 -N, --name string：账户的名称 -P, --parent string：此账户的父账户 -p, --partition strings：该账号可以访问的分区列表 -q, --qos_list strings：账号可以访问的qos列表 例：（添加账户PKU并添加PKU的子账户ComputingCentre） cacctmgr add account -N=PKU -D=school -p=CPU,GPU -q=test cacctmgr add account -N=ComputingCentre -D=department -P=PKU 1.3 添加用户 系统管理员可以添加任意账户的用户， 账户管理员可以添加同一账号下的新用户。添加的用户需要先有uid（先使用useradd在linux系统添加该用户）。 主要参数 -A/--account string：此用户所属的父账户 -c/--coordinate：设置用户是否为父账号的账户调度员（coordinator） -h/--help：帮助 -L/--level string：设置用户权限(none/operator/admin) (默认为 \"none\") -N/--name string：用户的名称 -p/--partition strings：该用户可以访问的分区列表 例 useradd CS cacctmgr add user -N=CS -A=PKU -p=CPU,GPU -L=admin # -p参数指明用户可用分区为CPU和GPU（分区必须同时为父账户PKU的可用分区），分区的allowed_qos_list与default_qos信息不支持指定，默认从父账户PKU中继承 cacctmgr add user -N=lab -A=ComputingCentre # 未指明-p参数，partition与qos信息都从父账户ComputingCentre中继承 2.删除 2.1. 删除用户 cacctmgr delete user -N lab 2.2 删除账户 仅系统管理员可以删除账户，删除账户时会检查账户下是否还有子账户或者用户，如果有则不允许删除，防止产生游离的用户，需要将其子账户和用户都设置在新账户下。 cacctmgr delete account -N ComputingCentre 2.3 删除qos cacctmgr delete qos -N test-qos 3.禁用 3.1 阻止用户或账户 主要命令 account：阻止账户 user：阻止账户下的用户 cacctmgr block user lab -A=ComputingCentre cacctmgr block account ComputingCentre 4.禁用 4.1 解除阻止用户或账户 主要命令 account：阻止账户 user：解除阻止账户下的用户 cacctmgr unblock user lab -A=ComputingCentre cacctmgr unblock account ComputingCentre 5. 查询 5.1 查找用户 所有用户均可以使用查询功能 cacctmgr find user lab cacctmgr find user CS cacctmgr show user 5.2 查找账户 cacctmgr find account ComputingCentre cacctmgr find account PKU cacctmgr show account 5.3 查找qos cacctmgr find qos test-qos cacctmgr show qos 6. 修改 6.1 修改账户 系统管理员可以修改任意信息，账户管理员可以修改本身账户的信息，但不能更改账户的父账户。 主要参数 --add_allowed_partition string：将新项添加到允许的分区列表 --add_allowed_qos_list strings：将新项添加到允许的qos列表 -Q/--default_qos string：修改账户默认qos --delete_allowed_partition string：从允许的分区列表中删除特定项目 --delete_allowed_qos_list strings：从允许的qos列表中删除特定项目 -D/--description string：修改账户的描述信息 -F/--force：强制操作 -h/--help：帮助 -N/--name string：需要进行修改的账户名称 --set_allowed_partition strings：设置允许的分区列表的内容 --set_allowed_qos_list strings：设置允许的qos列表的内容 例： cacctmgr modify account -N=ComputingCentre -D=\"Located in PKU\" 6.2 修改用户 系统管理员可以修改任意信息，账户管理员可以修改同账户下用户的信息，但不能更改用户的账户。 主要参数 -A/--account string：设置用户使用的帐号 --add_allowed_partition strings：将新项添加到允许的分区列表 --add_allowed_qos_list string：将新项添加到允许的qos列表 -L/--admin_level string：设置用户管理权限（none/operator/admin） -Q/--default_qos string：修改账户默认qos --delete_allowed_partition strings：从允许的分区列表中删除特定项目 --delete_allowed_qos_list string：从允许的qos列表中删除特定项目 -F/--force：强制操作 -h/--help：帮助 -N/--name string：需要进行修改的用户名称 -p/--partition string：被修改的分区，如果不显式设置该参数，默认修改所有分区 --set_allowed_partition strings：设置允许的分区列表的内容 --set_allowed_qos_list strings：设置允许的qos列表的内容 例： cacctmgr modify user -N=lab -A=ComputingCentre -L=operator --delete_allowed_partition GPU 6.3 修改qos 主要参数 -D/--description string：修改qos的描述信息 -h/--help：帮助 -c/--max_cpus_per_user uint32：(默认10) -J/--max_jobs_per_user uint32 -T/--max_time_limit_per_task uint：以秒为单位的时间（默认 3600） -N/--name string：需要进行修改的qos名称 -P/--priority uint32：(默认1000) 7. 显示 7.1 显示账户树 系统管理员会显示数据库所有根账户的账户树，账户管理员和用户会显示本身账户的账户树。 cacctmgr show accounts 7.2 显示用户 系统管理员会显示所有用户，账户管理员和用户会显示同一账户下的所有用户。 cacctmgr show users "},"command/cacct.html":{"url":"command/cacct.html","title":"4.9 cacct 查看作业信息","keywords":"","body":"cacct 查看作业信息 cacct可以查看队列中的作业信息。 查看集群中所有队列的作业信息（包括所有状态），默认输出100条信息。 cacct cacct运行结果展示 主要输出项 TaskId：作业号 TaskName：作业名 Partition：作业所在分区 Account：作业所属账户 AllocCPUs：作业分配的CPU数量 State：作业状态 ExitCode：作业状态码 主要参数 -A/--account string：指定查询作业的所属账户，指定多个账户时用逗号隔开。 -C/--config string：配置文件路径(默认为 \"/etc/crane/config.yaml\") -E/--endtime string：指定查询该时间之前结束的作业，例：cacct -E=2023-03-14T10:00:00 -o/--format string：以指定格式输出结果表，可以指定输出指定列以及列宽。 -F/-full：显示完整信息 -h/--help：显示帮助 -j/--job string：指定查询作业号，指定多个作业号时用逗号隔开。如 -j=2,3,4 --json：json格式输出命令执行结果 -m/--max-lines uint32：指定输出结果的最大条数。如-m=500表示最多输出500行查询结果 -n/--name string：指定查询作业名，指定多个作业名时用逗号隔开 -N/--noheader：输出隐藏表头 -p/--partition string：指定要查看的分区，多个分区名用逗号隔开，默认为全部 -S/--startime string：指定查询该时间之后开始的作业，例：cacct -S=2023-03-14T10:00:00 -t/--state string：指定要查看的作业状态，支持的状态：挂起（p）、正在运行（r）、已完成（c）、失败（f）、已取消（x）、超过时间限制（t），全部。（默认为“全部”） -s/--submit-time string：筛选提交时间在特定时间段内的作业，可以使用封闭间隔或半开放间隔 -u/--user string：指定查询某个用户的作业，指定多个用户时用逗号隔开。 -v/--version：查询版本号 例 cacct cacct -h cacct -N cacct -S=2024-07-22T10:00:00~2024-07-24T10:00:00 cacct -E=2024-07-22T10:00:00~2024-07-24T10:00:00 cacct -j=30618,30619,30620 cacct -u=cranetest cacct -A=CraneTest cacct -m=10 cacct -p GPU cacct -n=Test_Job cacct -o=\"%j %.10n %P %a %t\" cacct -A ROOT -m 10 cacct -m 10 -j 783925,783889 -t=c -F cacct -n test cacct -q test_qos cacct -m 10 -E=2024-10-08T10:00:00~2024-10-10T110:00:00 -p CPU -t c "}}